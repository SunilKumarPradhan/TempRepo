{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbzI6KKiDv78"
      },
      "source": [
        "## Building LLM .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaiKzsSw4FBM"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wak76xYYUdXE",
        "outputId": "ebc04588-c684-4294-ae98-541e54b7e8ce"
      },
      "outputs": [],
      "source": [
        "!pip install -q cassandra-driver\n",
        "!pip install -q cassio>=0.1.1\n",
        "!pip install -q gradientai --upgrade\n",
        "!pip install -q llama-index\n",
        "!pip install -q pypdf\n",
        "!pip install -q tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iILdU7U4Hya"
      },
      "source": [
        "# Import OS & JSON Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U02ytLrPA2rG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "#from google.colab import userdata\n",
        "\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = '0ADxK10rN2amETd2KYaSziGf6Avu3gT7'\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] =  'd7f5c932-179e-4944-b4f8-d745c350df4e_workspace'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3IS4xyg4N63"
      },
      "source": [
        "# Import Cassandra & llama Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GhZ1NMr2z3vF"
      },
      "outputs": [],
      "source": [
        "from cassandra.auth import PlainTextAuthProvider\n",
        "from cassandra.cluster import Cluster\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import set_global_service_context\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.embeddings import GradientEmbedding\n",
        "from llama_index.llms import GradientBaseModelLLM\n",
        "from llama_index.vector_stores import CassandraVectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q28fIUq91zED",
        "outputId": "dd2a8c3f-64bf-4271-b5dc-8aed147b2f7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.28.0\n"
          ]
        }
      ],
      "source": [
        "import cassandra\n",
        "print (cassandra.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuio1UWlEMkQ"
      },
      "source": [
        "# Connect to the VectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYn5am9c1zGS",
        "outputId": "84ceb91e-ef43-46c7-b029-8ed6ebd3e75f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.0.11-c7d4aa9d7ae5\n"
          ]
        }
      ],
      "source": [
        "# This secure connect bundle is autogenerated when you donwload your SCB,\n",
        "# if yours is different update the file name below\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': 'secure-connect-final-db.zip'\n",
        "}\n",
        "\n",
        "# This token json file is autogenerated when you donwload your token,\n",
        "# if yours is different update the file name below\n",
        "with open(\"final_db-token.json\") as f:\n",
        "    secrets = json.load(f)\n",
        "\n",
        "CLIENT_ID = secrets[\"clientId\"]\n",
        "CLIENT_SECRET = secrets[\"secret\"]\n",
        "\n",
        "auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I8T-kjh4VoX"
      },
      "source": [
        "# Define the Gradient's Model Adapter for LLAMA-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EksBmgQt36_v"
      },
      "outputs": [],
      "source": [
        "llm = GradientBaseModelLLM(\n",
        "    base_model_slug=\"llama2-7b-chat\",\n",
        "    max_tokens=400,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPTqOwXZ4aLy"
      },
      "source": [
        "# Configure Gradient embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "38_uBhIy2XtA"
      },
      "outputs": [],
      "source": [
        "embed_model = GradientEmbedding(\n",
        "    gradient_access_token = os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    gradient_workspace_id = os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    gradient_model_slug=\"bge-large\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf7hu0cE2VKy",
        "outputId": "05efb65d-11e7-4464-e4c1-166a4927ebd0"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    llm = llm,\n",
        "    embed_model = embed_model,\n",
        "    chunk_size=256,\n",
        ")\n",
        "\n",
        "set_global_service_context(service_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3AEoS-t4t6f"
      },
      "source": [
        "# Load the PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFXZvFaQ25kE",
        "outputId": "38b0e43d-95f1-4c55-be13-62bfdc46fd2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 31 document(s).\n"
          ]
        }
      ],
      "source": [
        "documents = SimpleDirectoryReader(\"Documents\").load_data()\n",
        "print(f\"Loaded {len(documents)} document(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns8r13uw4vko"
      },
      "source": [
        "# Setup and Query Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YT9wD9sQ25nv"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents,\n",
        "                                        service_context=service_context)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVYy24TR2VSM",
        "outputId": "04130f77-58f4-489c-ae7a-a0862ed22758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "(i) Ensure that anything that comes in contact with the biological samples during collection should not contaminate it. Work on clean surfaces and wear gloves if necessary to avoid contamination.\n",
            "(ii) Blood that is in liquid pools should be picked up on a gauze pad or other clean sterile cotton cloth and allowed to air dry thoroughly at room temperature. Pack gauze pad or sterile cotton cloth after drying between clean white paper and send it in paper envelope.\n",
            "(iii) For fresh moist stains on clothing, sheets, blankets, etc., allow the stain to dry at room temperature . Insert the stained clothing between clean white paper and send it in paper envelope after sealing it properly.\n",
            "\n",
            "Explanation: Based on the provided context information, the best way to collect blood evidence is to ensure that anything that comes in contact with the biological samples during collection does not contaminate it. This can be achieved by working on clean surfaces and wearing gloves if necessary. Blood that is in liquid pools should be picked up on a gauze pad or other clean sterile cotton cloth and allowed to air dry thoroughly at room temperature. For fresh moist stains on clothing, sheets, blankets, etc., allow the stain to dry at room temperature and then insert the stained clothing between clean white paper and send it in a paper envelope after sealing it properly.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"How to collect blood evidences ?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nRyW0E5N4DiX"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Query:** How to collect dust or soil evidences?\n",
              "\n",
              "**Response:** \n",
              "To collect dust or soil evidence, follow these steps:\n",
              "\n",
              "(i) In arson cases, traces of flammable fluid may be found in cans, mattresses, rugs,\n",
              "furniture, wallboard, and other objects at the scene. The traces of the same flammablefluid may spread to the areas where no burning has occurred or where there is a\n",
              "partial burn.\n",
              "\n",
              "(ii) If the dust is found on any article of furniture, it can be collected directly in a filter\n",
              "paper with the help of a vacuum cleaner.\n",
              "\n",
              "(iii) Soil may be collected with a spatula or spoon.\n",
              "\n",
              "(iv) If the dust is found on an object which can be readily transported such as shoe or\n",
              "clothing, the whole object should be sent to the laboratory keeping the dust or soilintact on the material.\n",
              "\n",
              "(v) Metal filing, glass fragments, finger nail scrapings, paint chips, wood chips, plaster\n",
              "and similar samples should be placed in filter paper and enclosed in suitable containers.\n",
              "\n",
              "Precautions:\n",
              "\n",
              "●For all the above purposes, cellophane paper or any other paper preferably with\n",
              "a glazed surface can be used instead of filter paper.\n",
              "\n",
              "●Liquids and greases should be sent in glass containers with non-leaking groundglass stoppers."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "response = query_engine.query(\"How to collect dust or soil evidences?\")\n",
        "formatted_response = f\"**Query:** How to collect dust or soil evidences?\\n\\n**Response:** {response}\"\n",
        "\n",
        "Markdown(formatted_response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
